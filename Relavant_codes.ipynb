{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PySpark SQL\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "#other modules\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "#Saving/reading data to/from disk\n",
    "import dill\n",
    "#python dataframe module\n",
    "import pandas as pd\n",
    "#Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "#Geo-data plotting\n",
    "import geojson\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Spark\n",
    "#Create a SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(\"local[*]\", \"pyspark_df\")\n",
    "\n",
    "#Create a SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read 18-month records during 201702-201807 using Spark\n",
    "df_grouped = []\n",
    "for n in [201702,201703,201704,201705,201706,201707,201708,201709,201710,201711,201712,201801,201802,201803,201804,201805,201806,201807]:\n",
    "    records = sc.textFile(localpath('data/PDPI/'+str(n)+'/'))\n",
    "    df_rec = records.map(lambda line: line.split(','))\\\n",
    "                    .filter(lambda u: u[1] != 'PCT')\\\n",
    "                    .map(lambda u: (u[2],float(u[6]),int(u[9]))).toDF()\n",
    "    df_rec = df_rec.withColumnRenamed(\"_1\", \"Practice_Code\")\\\n",
    "                   .withColumnRenamed(\"_2\", \"NIC\")\\\n",
    "                   .withColumnRenamed(\"_3\", \"Date\")\n",
    "    #print(df_rec.head())\n",
    "    if df_grouped == []:\n",
    "        df_grouped = df_rec.groupBy(['Practice_Code','Date'])\\\n",
    "                           .agg(fn.count('NIC').alias('Count'),fn.sum('NIC').alias('NIC_sum'))\n",
    "    else:\n",
    "        df_grouped = df_grouped.union(\\\n",
    "                                      df_rec.groupBy(['Practice_Code','Date'])\\\n",
    "                                     .agg(fn.count('NIC').alias('Count'),fn.sum('NIC').alias('NIC_sum'))\\\n",
    "                                     )\n",
    "\n",
    "#dill is an advanced version of pickle. Allows easy writing/reading data to/from disk.\n",
    "#Except for gathering info from the records, all dataframe manipulations were done using Pandas\n",
    "dill.dump(df_grouped.toPandas(), open('grouped_records_cost.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the dummy practices\n",
    "df_grouped = dill.load(open('grouped_records_cost.pkd', 'rb'))\n",
    "df_grouped.loc[df_grouped['Practice_Code'].apply(lambda s: 'Y' not in s)]\n",
    "dill.dump(df_grouped,open('grouped_records_cost.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the statistics with a GroupBy.\n",
    "df_grouped = dill.load(open('grouped_records_cost.pkd', 'rb'))\n",
    "df_stats_all = df_grouped.groupby('Practice_Code')['Count','NIC_sum']\\\n",
    "                .agg([np.median,np.var]).dropna()\n",
    "#Rename the columns\n",
    "df_stats_all.columns = [' '.join(col).strip() for col in df_stats_all.columns.values]\n",
    "#df_stats_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practices = sc.textFile(localpath('data/ADDR/'))\n",
    "\n",
    "df_prac_age = practices.map(lambda line: line.split(','))\\\n",
    "              .map(lambda u: (u[0].strip(),u[1].strip()))\\\n",
    "              .filter(lambda v: 'Y' not in v[1])\\\n",
    "              .toDF()\\\n",
    "              .withColumnRenamed('_1','Date').withColumnRenamed('_2','Practice_Code')\n",
    "df_prac_age = df_prac_age.groupBy('Practice_Code').count()\\\n",
    "                           .withColumnRenamed('count','Months')\n",
    "\n",
    "dill.dump(df_prac_age.toPandas(), open('df_prac_age.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means. Filter the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure all entry has 12-month worth of data\n",
    "df_prac_age = dill.load(open('df_prac_age.pkd', 'rb'))\n",
    "\n",
    "filter2 = pd.DataFrame()\n",
    "filter2['Months'] = df_grouped.groupby('Practice_Code')['Date'].count()\n",
    "filter2 = filter2.loc[filter2.Months ==18]\n",
    "\n",
    "#Join all filters\n",
    "df = df_stats_all.join(filter2,how='inner')\n",
    "df = df.drop(columns=['Months'])\n",
    "filtered_data = df.join(df_prac_age.set_index('Practice_Code'),how='inner').drop(columns=['Months'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform clustering analysis using scikit-learn's k-means class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Somewhat arbitary cut\n",
    "sample = filtered_data.loc[filtered_data['Count median'] >300]\n",
    "sample = sample.loc[sample['Count var'] < 6000].sort_values('Count var')\n",
    "y = sample['Count var'].tolist()\n",
    "\n",
    "estimator = KMeans(n_clusters=5).fit(np.asarray(y).reshape(-1, 1))\n",
    "print(estimator.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels generated are not ordered, so relabel them. Plot the histograms with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sample.index\n",
    "dict_label = {}\n",
    "for i in range(len(keys)):\n",
    "    dict_label[keys[i]] = int(estimator.labels_[i])\n",
    "\n",
    "df_labels = pd.DataFrame.from_dict(dict_label,orient='index',columns=['Label'])\n",
    "kmeans = sample.join(df_labels)\n",
    "\n",
    "#A dictionary for which kmeans label correspond to how many GPs\n",
    "dict_kcenter = {0:2,1:4,2:1,3:5,4:3}\n",
    "kmeans['nGP'] = kmeans['Label'].apply(lambda n: dict_kcenter[n])\n",
    "\n",
    "dill.dump(kmeans, open('df_kmeans.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the geo-location data. Plot the data to postcode areas using folium. The generated html files are interactive maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read address book of csv's and generate a table for geolocation of practices\n",
    "#I decided to use the Postcode areas for cleaner information\n",
    "\n",
    "def find_areacode(string):\n",
    "    s = ''\n",
    "    for x in string:\n",
    "        if x.isalpha():\n",
    "            s = s+x\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "#Create an RDD for the addr files\n",
    "practices = sc.textFile(localpath('data/ADDR/'))\n",
    "\n",
    "#Read lines, extract data and convert to Spark SQL dataframe\n",
    "df_geo = practices.map(lambda line: line.split(','))\\\n",
    "              .map(lambda u: (int(u[0]),u[1].strip(),find_areacode(u[7])))\\\n",
    "              .distinct().toDF()\n",
    "df_geo = df_geo.withColumnRenamed(\"_1\", \"Date\")\\\n",
    "               .withColumnRenamed(\"_2\", \"Practice_Code\").withColumnRenamed(\"_3\", \"Postcode_Area\")\n",
    "\n",
    "#In case of a practice with more than one location, keep the latest one.\n",
    "lastentry_udf_str = udf(lambda u: u[-1], StringType())#Define the udf\n",
    "df_geo = df_geo.orderBy(['Date','Practice_Code'])\\\n",
    "               .groupBy('Practice_Code').agg(fn.collect_list('Postcode_Area').alias('list'))\n",
    "df_geo = df_geo.withColumn('Postcode_Area',lastentry_udf_str(df_geo.list))\\\n",
    "               .select('Practice_Code','Postcode_Area')\n",
    "\n",
    "#Save to disk\n",
    "dill.dump(df_geo.toPandas(), open('df_geo.pkd', 'wb'))\n",
    "#df_geo = dill.load(open('df_geo.pkd', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
